My game Beat Catcher went through many, many, many transformations. In the brainstorming phases, I recall jumping from a shooter based game to playing with visual spawn logic and audio. I suppose I made some iteration of the latter, with my rhythm game. 
Initially, I created a bpm based system, where notes spawned at regular intervals. It worked, but it was basic and the sound library wasn’t actually analyzing anything. That was a hard reset where I ended up doing a lot of research on the minim library using links provided in class:
-https://www.generativehut.com/post/using-processing-for-music-visualization
-https://github.com/ddf/Minim/tree/main/examples/Basics
as well as this resource: 
https://code.compartmental.net/minim/
to understand the logic behind BeatDetect such as isKick and isSnare which became the center for how the code worked. This proposed a challenge later when I was converting to p5.js, but I’ll elaborate later.
In my restart, I focused purely on the spawn logic, creating a version that just had circles spawning and background flashing. BeatDetect using an FFT to divide the audio into frequency bands detected if there were low-frequency hz and spawned accordingly (the generativehut resource was especially useful for this). I originally had isHat, but it became too visually complicated. When I added specific music in, I just picked what mp3 files I already had on my computer. I noticed the notes were spawning when the sound was registered, not when the paddle would hit the notes. That’s when I added a delay, that still isn’t perfect with the complexity of the songs I chose, but it works. Then I started making it look better, starting by replacing the background flashing with these spotlights I made a class for. Each spotlight had its own tilt, height, width, and fade rate. To control their movement and rotation independently, I referred to the https://processing.org/tutorials/transform2d#the-transformation-matrix resource. I’m especially proud I got to use the blendMode(ADD) feature because it’s something I admired of some of the example works we took a look at back during the Kandinsky project. 
I refined the logic several times. Early on, spotlights and notes triggered too randomly, so I made it so a spotlight would only spawn if the bass was increased significantly compared to the previous frame. Similar to the note spawning, I had cooldowns as well. Then I made it more game related, having a game menu song game over song, and related pngs . 
This was the structure of my processing file, which was very time consuming so I thought it would be smoother from here. Translating it to p5.js was another story. It took me a while to understand that minim library was java based, so there wasn’t a perfect equivalent in javascript. I relied heavily on chatgpt in this section because it was taking too long to understand the differences with minim and p5.sound. From what I understand though, chat recreated the logic behind beatdetect and isKick functions using p5.FFT and p5.PeakDetect. This recreated some of the same rhythm behavior from my pde file, where note spawns and spotlights responded to the bass rather than random beats. It’s a bit different, and I do like my processing version a bit more, but I’m happy with how everything turned out regardless. I definitely became more comfortable using VS code and github as well, so this project definitely taught me a lot.
